#################################################################################
# common
spring.kafka.bootstrap-servers=172.0.198.101:9092,172.0.198.102:9092,172.0.198.103:9092
# listener
## 消费者手动提交偏移量
### RECORD: 当每一条记录被消费者监听器处理之后提交
### BATCH: 当每一批poll的数据被消费者监听器处理之后提交
### TIME: 当每一批poll的数据被消费者监听器处理之后,距离上次提交时间大于TIME时提交
### COUNT: 当每一批poll的数据被消费者监听器处理之后,被处理record数量大于等于COUNT时提交
### COUNT_TIME: TIME|COUNT有一个条件满足时提交
### MANUAL: poll拉取一批消息,处理完业务后,手动调用Acknowledgment.acknowledge()
### 先将offset存放到map本地缓存,在下一次poll之前从缓存拿出来批量提交
### MANUAL_IMMEDIATE: 手动调用Acknowledgment.acknowledge()后立即提交,一般使用这种
spring.kafka.listener.ack-mode=manual_immediate
## 推荐为分区数
## 在监听器容器中运行的线程数,创建多少个consumer,值必须小于等于Topic的分区数(推荐直接取分区数)
spring.kafka.listener.concurrency=1
spring.kafka.listener.poll-timeout=100ms
## 消费监听接口监听的主题不存在时,默认会报错
spring.kafka.listener.missing-topics-fatal=false
## 使用批量消费需要将listener的type设置为batch,该值默认为single
spring.kafka.listener.type=batch
# consumer
## 默认消费者组
spring.kafka.consumer.group-id=group_1
## 自动提交开关
spring.kafka.consumer.enable-auto-commit=false
## 自动提交间隔
## 自动提交的时间间隔(版本中这里采用的值的类型Duration需要符合特定的格式[如1S2M3H4D])
spring.kafka.consumer.auto-commit-interval=1s
## 没有偏移量的分区或者偏移量无效时如何处理
### earliest: 消费者将从起始位置读取分区的记录
### latest: 消费者将从最新的记录开始读取数据
### none: 只要有一个分区不存在已提交的offset,就抛出异常
spring.kafka.consumer.auto-offset-reset=earliest
## 请求返回记录数
spring.kafka.consumer.max-poll-records=2
## 这个参数允许消费者指定从broker读取消息时最小的Payload的字节数
## 当消费者从broker读取消息时,如果数据字节数小于这个阈值,broker会等待直到有足够的数据,然后才返回给消费者
## 对于写入量不高的主题来说,这个参数可以减少broker和消费者的压力,因为减少了往返的时间
## 而对于有大量消费者的主题来说,则可以明显减轻broker压力
spring.kafka.consumer.fetch-min-size=1
## fetch.min.bytes参数指定了消费者读取的最小数据量,而这个参数则指定了消费者读取时最长等待时间
## 从而避免长时间阻塞,这个参数默认为500ms
spring.kafka.consumer.fetch-max-wait=500ms
## 反序列化
spring.kafka.consumer.key-deserializer=org.apache.kafka.common.serialization.StringDeserializer
spring.kafka.consumer.value-deserializer=org.apache.kafka.common.serialization.StringDeserializer
## 从服务器获取最小字节数
spring.kafka.consumer.properties.fetch.min.bytes=102400
## 消费者请求超时时间
spring.kafka.consumer.properties.request.timeout.ms=1000
## 会话超时时间
spring.kafka.consumer.properties.session.timeout.ms=120000
## 心跳频率
spring.kafka.consumer.properties.heartbeat.interval.ms=40000
# producer
## 发送消息应答确认成功机制
## 0: 生产者发送过来的数据,不需要等数据落盘应答
## 1: 生产者发送过来的数据,Leader收到数据后应答
## all/-1: 只有当所有参与复制的节点全部收到消息时,生产者才会收到一个来自服务器的成功响应
spring.kafka.producer.acks=all
## 发生错误后,消息重发的次数,0为不启用重试机制,默认为Integer最大值
spring.kafka.producer.retries=10
## 重试之间的间隔毫秒
spring.kafka.producer.properties.retry.backoff.ms=100
## 单个连接上发送的未响应请求个数(1保证不乱序)
spring.kafka.producer.properties.max.in.flight.requests.per.connection=1
## 不允许非ISR中的副本被选举为leader,以避免数据丢失
spring.kafka.producer.properties.unclean.leader.election.enable=false
## 每个Partition的副本大于等于3
spring.kafka.producer.properties.replication.factor=3
## 被写入多少副本才算成功(设置factor-1比较合理,避免挂掉一个无法发送消息)
spring.kafka.producer.properties.min.insync.replicas=2
## 序列化器
spring.kafka.producer.key-serializer=org.apache.kafka.common.serialization.StringSerializer
spring.kafka.producer.value-serializer=org.apache.kafka.common.serialization.StringSerializer
## 批次大小(字节)
## 当有多个消息需要被发送到统一分区时,生产者会把他们放在同一批次里
## 该参数指定了一个批次可以使用的内存大小,按照字节数计算,默认16K
spring.kafka.producer.batch-size=16384
## 等待发送到服务器的记录的最大缓冲字节(默认32M)
spring.kafka.producer.buffer-memory=33554432
## 消息压缩算法(none/gzip/snappy/lz4/zstd)默认none
spring.kafka.producer.compression-type=none
## 自定义分区器
#spring.kafka.producer.properties.partitioner.class=org.apache.kafka.clients.producer.internals.DefaultPartitioner
## 发送数据是等待响应时间
spring.kafka.producer.properties.request.timeout.ms=1000
## 批量发送延迟为10毫秒,启用该功能能有效减少生产者发送消息次数,从而提高并发量(默认0)
spring.kafka.producer.properties.linger.ms=10
## 开启事务
#spring.kafka.producer.properties.transaction-id-prefix: APP
